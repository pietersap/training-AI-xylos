{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TensorFlow_Eager.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"python352","language":"python","name":"python352"}},"cells":[{"metadata":{"colab_type":"text","id":"view-in-github"},"cell_type":"markdown","source":["[View in Colaboratory](https://colab.research.google.com/github/gbaeke/xylosai/blob/master/intro/TensorFlow_Eager.ipynb)"]},{"metadata":{"id":"bfVfJz0XiFqT","colab_type":"text"},"cell_type":"markdown","source":["# 0 Eager execution\n","\n","This notebook section follows the same structure as the eager execution guide on the TensorFlow website.   \n","https://www.tensorflow.org/guide/eager\n","\n","## 0.1 Basics"]},{"metadata":{"id":"OSyh7THviFqV","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","tf.enable_eager_execution()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-TENNKHBiFqb","colab_type":"code","colab":{}},"cell_type":"code","source":["#x = tf.Variable(2) # not available when eager execution is enabled!"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IwLdnQaEiFqi","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow.contrib.eager as tfe"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OurzE2v2iFqm","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qdE5qIqMiFqr","colab_type":"code","outputId":"4f60a2ff-ac41-4752-b21d-bbe7eb4e73e1","colab":{}},"cell_type":"code","source":["vector_a = tf.constant([3,4])\n","vector_b = tf.constant([1,2])\n","\n","print(vector_a)\n","print(vector_b)\n","\n","vector_c = vector_a + vector_b\n","vector_cc = tf.add(vector_a,vector_b) #equivalent\n","vector_ccc = np.add(vector_a,vector_b)\n","\n","print(vector_c)\n","print(vector_cc)\n","\n","print(vector_ccc)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor([3 4], shape=(2,), dtype=int32)\n","tf.Tensor([1 2], shape=(2,), dtype=int32)\n","tf.Tensor([4 6], shape=(2,), dtype=int32)\n","tf.Tensor([4 6], shape=(2,), dtype=int32)\n","[4 6]\n"],"name":"stdout"}]},{"metadata":{"id":"9Ll-MbjWiFq3","colab_type":"code","outputId":"9e398aa1-6296-44c3-de37-bd6f44faedbf","colab":{}},"cell_type":"code","source":["print(vector_a.numpy())\n","print(vector_b.numpy())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[3 4]\n","[1 2]\n"],"name":"stdout"}]},{"metadata":{"id":"I4cFaIooiFq_","colab_type":"code","outputId":"ab2c00f0-86ad-4805-a966-e6764e2be71f","colab":{}},"cell_type":"code","source":["a = tf.constant(0)\n","print(a)\n","print(a + tf.constant(1))\n","print(a+1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor(0, shape=(), dtype=int32)\n","tf.Tensor(1, shape=(), dtype=int32)\n","tf.Tensor(1, shape=(), dtype=int32)\n"],"name":"stdout"}]},{"metadata":{"id":"4pEij01DiFrF","colab_type":"text"},"cell_type":"markdown","source":["**conclusion** \n"," \n"," 1. Eager execution is imperative. TensorFlow operations evaluate and return values immediately (no session.run()). This means you can write code in an intuitive and imperative way. It also makes debugging easier.\n"," 2. tf.Tensor objects reference concrete values instead of symbolic handles to nodes in a computational graph. \n"," 3. Nice integration with NumPy"]},{"metadata":{"id":"huroOsvFiFrH","colab_type":"text"},"cell_type":"markdown","source":["## 0.2 Dynamic control flow"]},{"metadata":{"id":"M7lPbY_5iFrJ","colab_type":"code","outputId":"c320b7f3-4fe3-433e-8dca-d087030373f4","colab":{}},"cell_type":"code","source":["#Summing all even numbers between 0 and 100\n","\n","tf_i = tf.constant(0)\n","tf_stop = tf.constant(100)\n","tf_sum = tf.constant(0)\n","\n","while tf_i.numpy() <= 100:\n","    if int(tf_i % 2) == 0:\n","        tf_sum = tf_sum + tf_i\n","    tf_i = tf_i + 1\n","    \n","print(tf_sum)\n","print(tf_sum.numpy())\n","\n","    \n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor(2550, shape=(), dtype=int32)\n","2550\n"],"name":"stdout"}]},{"metadata":{"id":"zTgB3qEXiFrQ","colab_type":"text"},"cell_type":"markdown","source":["**conclusion**  \n","All functionality of the host language  is available while executing. This allows, for example, to write conditionals based on the tensor values."]},{"metadata":{"id":"K5CuGfNNiFrS","colab_type":"text"},"cell_type":"markdown","source":["## 0.3 Eager training\n","\n","The power of machine learning frameworks like TensorFlow comes from **Automatic differentiation**. This allows to compute gradients easily during **backpropagation** when training a neural network with gradient descent.\n","\n","In the original TensorFlow, a computational graph was fully built and then run afterwards. When running a graph that contains an optimizer (which involves backpropagation), TensorFlow keeps track of gradients, at every step of the graph, and uses them to update the parameters\n","\n","In eager execution, the calculations are made \"on the fly\". A **tf.GradientTape** is used to trace operations for computing gradients later. All forward-pass operations are recorded to the tape. To compute the gradient, play the tape backwards and then discard. A tf.GradientTape can only compute one gradient, subsequent calls throw a runtime error."]},{"metadata":{"id":"SQx80_3wiFrT","colab_type":"code","outputId":"cd48a5e8-c4f5-410b-b583-72ca86baa40c","colab":{}},"cell_type":"code","source":["x = tfe.Variable(2.0) #notice the float type\n","\n","with tf.GradientTape() as tape:\n","    loss = x**3\n","    loss_a = a**3\n","    \n","grad = tape.gradient(loss, x)\n","print(grad)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor(12.0, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"metadata":{"id":"YU6qKqyRiFrZ","colab_type":"text"},"cell_type":"markdown","source":["Below is a simple training loop for linear regression, using the tf.GradientTape. First, a toy dataset of 1000 points around 3 \\* x + 2 is generated. Then, the weight (W) and bias (B) are estimated with gradient descent. The estimated W and B should be close to 3 and 2 respectively. "]},{"metadata":{"id":"3ZPR9TymiFrb","colab_type":"code","outputId":"c5a0ca3b-1516-418a-c4fd-cb66661829c6","colab":{}},"cell_type":"code","source":["# A toy dataset of points around 3 * x + 2\n","NUM_EXAMPLES = 1000\n","training_inputs = tf.random_normal([NUM_EXAMPLES])\n","noise = tf.random_normal([NUM_EXAMPLES])\n","training_outputs = training_inputs * 3 + 2 + noise\n","\n","def prediction(input, weight, bias):\n","    return input * weight + bias\n","\n","# A loss function using mean-squared error\n","def loss(weights, biases):\n","    error = prediction(training_inputs, weights, biases) - training_outputs\n","    return tf.reduce_mean(tf.square(error))\n","\n","# Return the derivative of loss with respect to weight and bias\n","def grad(weights, biases):\n","    with tf.GradientTape() as tape:\n","        loss_value = loss(weights, biases)\n","    return tape.gradient(loss_value, [weights, biases])\n","\n","train_steps = 200\n","learning_rate = 0.01\n","# Start with arbitrary values for W and B on the same batch of data\n","W = tfe.Variable(5.)\n","B = tfe.Variable(10.)\n","\n","print(\"Initial loss: {:.3f}\".format(loss(W, B)))\n","\n","for i in range(train_steps):\n","    dW, dB = grad(W, B)\n","    # a tfe.Variable object has the handy assign_sub method. See next section.\n","    W.assign_sub(dW * learning_rate)\n","    B.assign_sub(dB * learning_rate)\n","    if i % 20 == 0:\n","        print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))\n","\n","print(\"Final loss: {:.3f}\".format(loss(W, B)))\n","print(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initial loss: 69.241\n","Loss at step 000: 66.524\n","Loss at step 020: 30.072\n","Loss at step 040: 13.901\n","Loss at step 060: 6.727\n","Loss at step 080: 3.544\n","Loss at step 100: 2.131\n","Loss at step 120: 1.505\n","Loss at step 140: 1.226\n","Loss at step 160: 1.103\n","Loss at step 180: 1.048\n","Final loss: 1.025\n","W = 3.026477098464966, B = 2.1484572887420654\n"],"name":"stdout"}]},{"metadata":{"id":"diFh9gGQiFrj","colab_type":"text"},"cell_type":"markdown","source":["Success! After 200 steps, the estimated line is 2.99 \\* X + 2.122. This is pretty close to the original 3 \\* x + 2."]},{"metadata":{"id":"aNnuGl77iFrl","colab_type":"text"},"cell_type":"markdown","source":["## 0.4 About variables in eager execution: tfe.Variable()\n","\n","Variables are objects! During eager execution, variables persist until the last reference to the object is removed, and is then deleted. Sharing variables means simply reusing variables. No need to worry about variable scopes etc. tfe.Variable objects store **mutable** tf.Tensor values. \n","\n","Thus, TensorFlow eager is more object-oriented and pythonic. Read more on the [TensorFlow guide on eager execution](https://www.tensorflow.org/guide/eager)"]},{"metadata":{"id":"LI4gO6DQiFro","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.device(\"gpu:0\"):\n","    v = tfe.Variable(tf.random_normal([1000, 1000]))\n","    v = None  # v no longer takes up GPU memory"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fvF5PtZiiFrs","colab_type":"text"},"cell_type":"markdown","source":["## 0.5 Working with graphs\n","\n","We have seen the advantages of eager execution in action. Writing graph code was more complex and more difficult to debug. \n","\n","But **graph execution** still has important advantages over eager execution. It is better for distributed training, performance optimizations, and - most importantly - **deployment**.\n","\n","From the TensorFlow guide:  \n","\n","\"  \n","For building and training graph-constructed models, the Python program first builds a graph representing the computation, then invokes Session.run to send the graph for execution on the C++-based runtime. This provides:\n","\n","1. Automatic differentiation using static autodiff.\n","2. Simple deployment to a platform independent server.\n","3. Graph-based optimizations (common subexpression elimination, constant-folding, etc.).\n","4. Compilation and kernel fusion.\n","5. Automatic distribution and replication (placing nodes on the distributed system).  \n","\n","Deploying code written for eager execution is more difficult: either generate a graph from the model, or run the Python runtime and code directly on the server\n","\n","The same code written for eager execution will also build a graph during graph execution. Do this by simply running the same code in a new Python session where eager execution is not enabled. \n","\"\n","\n","Write compatible code! Write code that is going to work both ways. Tip: use the Keras API. From the video:\n","\n","\n","Refer to the resources below for more details and ideas.\n","\n","Conversely, it is possible to force eager execution inside a graph session. This can be used, for example, when there is a specific piece of code that you don't know how to implement in the graph way. Use **tfe.py_func**, a wrapper for eager python functions. "]},{"metadata":{"id":"CapSEcJDiFru","colab_type":"code","outputId":"8c6322aa-e0ce-4cfb-fe2a-df732114ebb1","colab":{}},"cell_type":"code","source":["#To see that this code actually works, restart the Jupyter kernel (Note that you will lose all existing variables).\n","#notice that tf.enable_eager_execution() is not called.\n","\n","import tensorflow as tf\n","import tensorflow.contrib.eager as tfe\n","\n","def my_py_func(x):\n","    x = tf.matmul(x, x)  # You can use tf ops\n","    print(x)  # but it's eager!\n","    print(x.numpy())\n","    if int(x.numpy()) > 3:\n","        print(\"the result is higher than 3\")\n","    return x\n","\n","with tf.Session() as sess:\n","    x = tf.placeholder(dtype=tf.float32)\n","    # Call eager function in graph!\n","    pf = tfe.py_func(my_py_func, [x], tf.float32)\n","    sess.run(pf, feed_dict={x: [[2.0]]})  # [[4.0]]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n","[[4.]]\n","the result is higher than 3\n"],"name":"stdout"}]},{"metadata":{"id":"03UitpuNiFr4","colab_type":"text"},"cell_type":"markdown","source":["Inside the eager function my_py_func, you can do all \"eager things\" like inspecting values and using the dynamic control flow with conditionals."]},{"metadata":{"id":"FIljWsmIiFr7","colab_type":"text"},"cell_type":"markdown","source":["## 0.6 Resources\n","\n","This notebook was inspired on the [TensorFlow guide on eager execution](https://www.tensorflow.org/guide/eager), found on the official website, and a [video from the TensorFlow Dev Summit 2018](https://www.youtube.com/watch?v=T8AW0fKP0Hs). Follow the links to learn more about eager execution!\n","\n","\n","\n"]},{"metadata":{"id":"Z0cCjSZhiFr9","colab_type":"text"},"cell_type":"markdown","source":["# Eager basics"]},{"metadata":{"colab_type":"text","id":"L0zxG9QWV93k"},"cell_type":"markdown","source":["Eager execution is an imperative programming environment. There is no need to build a graph and then execute it. Operations return concrete values.\n","\n","For more details also check [this](https://towardsdatascience.com/eager-execution-tensorflow-8042128ca7be)"]},{"metadata":{"colab_type":"code","id":"EjgfHmcHVwng","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","\n","tf.enable_eager_execution()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"ZvNrGb6mWmRj","outputId":"9848c274-71ab-464e-9554-f41eafdb1566","colab":{"base_uri":"https://localhost:8080/","height":33}},"cell_type":"code","source":["tf.executing_eagerly() # should return true"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"metadata":{"colab_type":"code","id":"FLDARGAxWq9x","colab":{}},"cell_type":"code","source":["vector_a = tf.constant([3,4])\n","vector_b = tf.constant([1,2])"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"VJhRcqOTXLMb","outputId":"49c4ff81-0cc4-48e1-a62f-7797d2332279","colab":{"base_uri":"https://localhost:8080/","height":33}},"cell_type":"code","source":["total = vector_a + vector_b\n","print(total) # the output operation is the actual value and not the tensor"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor([4 6], shape=(2,), dtype=int32)\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"LHoKFp-jXN-u","colab":{}},"cell_type":"code","source":["# tensors of different ranks\n","# rank 0 (just a scalar)\n","my_scalar = tf.constant(\"Elephant\", tf.string)\n","\n","# rank 1 (vector)\n","my_vector = tf.constant([1,2,3,4])\n","\n","# rank 2 (matrix)\n","my_matrix = tf.constant([[1,2], [3,4]])\n","\n","\n","# rank 3 (cube, e.g. x and y coordinates for pixels + RGB values)\n","my_cube = tf.constant([ [[1,2],[3,4]], [[5,6], [7,8]] ])"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"kWNApqqzaSR5","outputId":"ada25308-8ca9-4593-c25e-a047a9cdc22f","colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":["# print the ranks\n","print(tf.rank(my_scalar), tf.rank(my_vector), tf.rank(my_matrix), tf.rank(my_cube))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(2, shape=(), dtype=int32) tf.Tensor(3, shape=(), dtype=int32)\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"MSaFuPl3bzNK","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"notebook_5_flight_delay_decisiontree.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"oIKjnuqmXEYt","colab_type":"text"},"cell_type":"markdown","source":["# 0 Imports and helper functions"]},{"metadata":{"id":"OjgruUDIW4Zk","colab_type":"code","colab":{}},"cell_type":"code","source":["import sklearn\n","import pandas as pd\n","from sklearn import tree\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import time\n","from sklearn.metrics import roc_curve, auc\n","import json\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AgTursLWXLOz","colab_type":"text"},"cell_type":"markdown","source":["# 1 Loading the data\n","\n","In this notebook we build a decision tree binary classifier to predict wether or not a flight will be delayed. It is based on the same dataset as in notebook 4, where we built a binary classifier with logistic regression. All data preprocessing steps are the same, and we will simply directly import the processed dataset containing the joined flight, delay and weather information."]},{"metadata":{"id":"znHa20w5XJwD","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gd23xNJlXaec","colab_type":"code","colab":{}},"cell_type":"code","source":["delays = pd.read_csv(\"/content/drive/My Drive/xylosai/flightDelay/FlightDelaysData_clean.csv\",header=0,index_col = 0)\n","delays.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mQUhxPabcSaa","colab_type":"code","colab":{}},"cell_type":"code","source":["delays.info(memory_usage='deep',max_cols=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Mpz06o_taQ8R","colab_type":"code","colab":{}},"cell_type":"code","source":["# just quickly downcasting all int64 columns (memory)\n","int_columns = delays.dtypes[delays.dtypes == 'int64'].index.tolist()\n","counter = 0\n","for columnname in int_columns:\n","  counter += 1\n","  if (counter%5 == 0):\n","    print(\"downcasting column {0} of {1}\".format(counter,len(int_columns)))\n","  delays[columnname] = pd.to_numeric(delays[columnname],downcast=\"integer\")\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"xuE2KTOkdGiz","colab_type":"code","colab":{}},"cell_type":"code","source":["delays.info(memory_usage='deep',max_cols=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"quGGntXEswx1","colab_type":"text"},"cell_type":"markdown","source":["# 2 Decision tree classifier"]},{"metadata":{"id":"EnC_ZDavLxfg","colab_type":"text"},"cell_type":"markdown","source":["## 2.1 Building a first model\n","\n","With a decision tree classifier, we can learn non-linear decision boundaries.\n","We keep the wind direction input. As a reminder, let's look at our final dataset."]},{"metadata":{"id":"vqZl6bZDbr9y","colab_type":"code","colab":{}},"cell_type":"code","source":["delays.columns"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gLR875rKLxfX","colab_type":"code","colab":{}},"cell_type":"code","source":["Y = delays[\"ArrDel15\"]\n","del delays[\"ArrDel15\"]\n","X = delays\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zGnBv2BYLxfe","colab_type":"code","colab":{}},"cell_type":"code","source":["X_traindev, X_test, Y_traindev, Y_test = train_test_split(X,Y,random_state=0,test_size = 0.10)\n","\n","del X\n","del Y\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OkLq70Rwee3y","colab_type":"code","colab":{}},"cell_type":"code","source":["# balancing the classes in the train/dev set, memory-optimized\n","\n","l_1 = np.sum(Y_traindev == 1)\n","\n","\n","X_traindev = pd.concat([X_traindev[Y_traindev == 1], X_traindev[Y_traindev == 0][0:l_1]],axis=0)\n","Y_traindev = pd.concat([Y_traindev[Y_traindev == 1], Y_traindev[Y_traindev == 0][0:l_1]],axis=0)\n","\n","X_traindev, Y_traindev = sklearn.utils.shuffle(X_traindev, Y_traindev, random_state=0)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-51uHx6VfIV0","colab_type":"code","colab":{}},"cell_type":"code","source":["# category count in the train/dev set after balancing. Should be equal.\n","print(\"not delayed: {0}\".format(np.sum(Y_traindev == 0)))\n","print(\"delayed: {0}\".format(np.sum(Y_traindev == 1)))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oD16wwHceS8_","colab_type":"code","colab":{}},"cell_type":"code","source":["X_train, X_dev, Y_train, Y_dev = train_test_split(X_traindev,Y_traindev,random_state=0,test_size = 0.10)\n","\n","del X_traindev\n","del Y_traindev"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-SxLgxRELxfj","colab_type":"code","colab":{}},"cell_type":"code","source":["clf = tree.DecisionTreeClassifier(random_state=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kJXoRQniLxfl","colab_type":"code","colab":{}},"cell_type":"code","source":["start = time.time()\n","clf.fit(X_train, Y_train)\n","end = time.time()\n","\n","seconds = end-start\n","print(\"Fitting took {0} seconds\".format(seconds))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xHFyXadsiYN3","colab_type":"text"},"cell_type":"markdown","source":["## 2.2 Evaluating our model\n","\n","Let's first look at the predicted probabilities for our test set... Somethings odd..."]},{"metadata":{"id":"lTMtxtbXLxfq","colab_type":"code","colab":{}},"cell_type":"code","source":["Y_test_prob = clf.predict_proba(X_test)\n","Y_test_prob\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2aNY5aNnLxf8","colab_type":"code","colab":{}},"cell_type":"code","source":["clf.tree_.node_count #that's a lot of nodes!"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6HliJ0kCLxf9","colab_type":"code","colab":{}},"cell_type":"code","source":["auc = sklearn.metrics.roc_auc_score(Y_test,Y_test_prob[:,1])\n","print(auc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nJUDhf-tigHp","colab_type":"code","colab":{}},"cell_type":"code","source":["fpr_test, tpr_test, tresholds = sklearn.metrics.roc_curve(Y_test, Y_test_prob[:,1])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wYHILomJjFVp","colab_type":"code","colab":{}},"cell_type":"code","source":["def plotROC(fpr, tpr):\n","  fig = plt.figure(figsize = (10,10))\n","  plt.xlabel(\"false positive rate (FPR)\",fontsize = 15)\n","  plt.ylabel(\"true positive rate (TPR)\",fontsize = 15)\n","  plt.title(\"ROC curve\",fontsize=20)\n","  plt.plot(fpr, tpr,\"b\",fpr, fpr, \"k:\")\n","  plt.legend((\"ROC curve\",\"baseline\"),fontsize=15)\n","  plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DmoG8eysjFRI","colab_type":"code","colab":{}},"cell_type":"code","source":["plotROC(fpr_test, tpr_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2sTZLqa2p-XU","colab_type":"code","colab":{}},"cell_type":"code","source":["Y_train_prob = clf.predict_proba(X_train)\n","\n","auc = sklearn.metrics.roc_auc_score(Y_train,Y_train_prob[:,1])\n","print(auc)\n","\n","fpr_train, tpr_train, tresholds = sklearn.metrics.roc_curve(Y_train, Y_train_prob[:,1])\n","\n","plotROC(fpr_train, tpr_train)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nxLGmO-_rqU4","colab_type":"text"},"cell_type":"markdown","source":["**What's wrong here?**"]},{"metadata":{"id":"sLrx1eR0LxgC","colab_type":"text"},"cell_type":"markdown","source":["## 2.3 Regularization\n","\n","\n","In the code above, we have not limited the growth of our tree in any way. As a result, the tree has kept making splits untill every node had zero impurities. The result is a tree that is overfitted on the data.\n","\n","For decision trees, the growth of the tree must be limited to avoid overfitting. This can be done in multiple ways. In the next section we will limited the tree growth by:\n","\n","1. Limiting the maximum depth of the tree\n","2. Setting a minimum amount off samples required to allow another split of a tree stump\n","\n","This introduces two hyperparameters: \n","\n","1. The maximum depth\n","2. The minimum amount of samples to split\n","\n","We will use both techniques at the same time with fixed values for the hyperparameters. \n","\n","In a more advanced scenario, one could use both techniques together with different combinations of hyperparameters to obtain the global optimal set of hyperparameters. In section 4.3 we plot the results for different values of hyperparameters. Since this code requires some time to complete, the resulting figures are included in the notebook so you don't have to run the code again. \n","\n"]},{"metadata":{"id":"BUmtaHHNxmWi","colab_type":"code","colab":{}},"cell_type":"code","source":["  depth = 10\n","  split_amount = 300\n","  \n","  clf = tree.DecisionTreeClassifier(random_state=0, max_depth = depth, min_samples_leaf = split_amount)\n","  clf.fit(X_train, Y_train)\n","  Y_test_prob = clf.predict_proba(X_test)\n","  auc = sklearn.metrics.roc_auc_score(Y_test,Y_test_prob[:,1])\n","  print(\"AUC of this regularized tree: {0}\".format(auc))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x5kHoT2gxeJa","colab_type":"text"},"cell_type":"markdown","source":["## 2.4 Hyperparameter search\n","\n","**NOTE:** Hyperparameter tuning involves the DEV (validation) set. "]},{"metadata":{"id":"CrUbw_hTsIbp","colab_type":"text"},"cell_type":"markdown","source":["### Hyper parameter search: limiting the maximum tree depth\n","\n"]},{"metadata":{"id":"17y1Hz6QLxgC","colab_type":"code","colab":{}},"cell_type":"code","source":["depths = [1,3,5,8,10,20,30,40]\n","\n","auc_results = []\n","\n","for depth in depths:\n","  print(\"fitting with depth = {}...\".format(depth))\n","  clf = tree.DecisionTreeClassifier(random_state=0, max_depth = depth)\n","  clf.fit(X_train, Y_train)\n","  Y_dev_prob = clf.predict_proba(X_dev)\n","  auc = sklearn.metrics.roc_auc_score(Y_dev,Y_dev_prob[:,1])\n","  auc_results.append(auc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"94S2ib8eyh7P","colab_type":"text"},"cell_type":"markdown","source":["**Watch the result** [here](https://drive.google.com/open?id=19VYv95yLx9beSdoxyn5NenC7p2Uqmox1)\n","\n"]},{"metadata":{"id":"ZTM16m1ktmhl","colab_type":"code","colab":{}},"cell_type":"code","source":["  fig = plt.figure(figsize = (10,10))\n","  plt.xlabel(\"maximum tree depth\",fontsize = 15)\n","  plt.ylabel(\"Area Under the Curve (AUC)\",fontsize = 15)\n","  plt.title(\"Limiting tree growth (regularization)\",fontsize=20)\n","  plt.plot(depths, auc_results, \"b\")\n","  plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YXhxTUhdVu4Y","colab_type":"text"},"cell_type":"markdown","source":["It appears that the best validation error is for a depth around 10. Let's evaluate the TEST error for this depth.\n","\n","**Question:** How will the AUC versus tree depth look when evaluating on the **training set**?"]},{"metadata":{"id":"1FHH0iE7Vvuo","colab_type":"code","colab":{}},"cell_type":"code","source":["clf = tree.DecisionTreeClassifier(random_state=0, max_depth = 10)\n","clf.fit(X_train, Y_train)\n","\n","Y_test_prob = clf.predict_proba(X_test)\n","\n","auc = sklearn.metrics.roc_auc_score(Y_test,Y_test_prob[:,1])\n","\n","print(\"test AUC: {0}\".format(auc))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jkl3oJNjsMQn","colab_type":"text"},"cell_type":"markdown","source":["### Hyperparameter search: minimum amount of samples to split"]},{"metadata":{"id":"3V9b0z0YsUK2","colab_type":"code","colab":{}},"cell_type":"code","source":["split_amounts = [2,300,500,750,1500]\n","\n","auc_results = []\n","\n","for split_amount in split_amounts:\n","  print(\"fitting with minimum {} samples before splitting..\".format(split_amount))\n","  clf = tree.DecisionTreeClassifier(random_state=0, min_samples_leaf = split_amount)\n","  clf.fit(X_train, Y_train)\n","  Y_dev_prob = clf.predict_proba(X_dev)\n","  auc = sklearn.metrics.roc_auc_score(Y_dev,Y_dev_prob[:,1])\n","  auc_results.append(auc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wR79tCrg0d2w","colab_type":"text"},"cell_type":"markdown","source":["**Watch the result** [here](https://drive.google.com/open?id=1nZV9R8kGMq3_c_VEIWtObSh9yi4kUzDd)\n","\n","**Exercise:** What is the best minimum amount of samples to split? Find the TEST error for this model.\n","\n"]},{"metadata":{"id":"1NOG62PSuXcn","colab_type":"code","colab":{}},"cell_type":"code","source":["  fig = plt.figure(figsize = (10,10))\n","  plt.xlabel(\"minimum samples per leaf\",fontsize = 15)\n","  plt.ylabel(\"Area Under the Curve (AUC)\",fontsize = 15)\n","  plt.title(\"Limiting tree growth (regularization)\",fontsize=20)\n","  plt.plot(split_amounts, auc_results,\"b\")\n","  plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UoH_Xw1B2R1s","colab_type":"text"},"cell_type":"markdown","source":["# 3 Boosted tree\n","\n","We will training a model of boosted decision stumps. The base classifier is a tree with depth = 1"]},{"metadata":{"id":"Z2MYQdOnRfrh","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.ensemble import AdaBoostClassifier"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0pyZCHhT2UyC","colab_type":"code","colab":{}},"cell_type":"code","source":["base_classifier = tree.DecisionTreeClassifier(random_state=0, max_depth = 1)\n","adaboost_classifier = AdaBoostClassifier(base_classifier, 50)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"M6LzomXsRxHE","colab_type":"code","colab":{}},"cell_type":"code","source":["adaboost_classifier.fit(X_train, Y_train)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ionQNSMcR40t","colab_type":"code","colab":{}},"cell_type":"code","source":["Y_test_prob = adaboost_classifier.predict_proba(X_test)\n","\n","auc = sklearn.metrics.roc_auc_score(Y_test,Y_test_prob[:,1])\n","\n","print(\"test AUC: {0}\".format(auc))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-l10eaARTzoN","colab_type":"text"},"cell_type":"markdown","source":["We will leave the hyperparamter tuning of T as an exercise."]}]}
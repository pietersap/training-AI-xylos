{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"classifier-eager.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"colab_type":"text","id":"d7KmrKX8teL0"},"cell_type":"markdown","source":["# 1 Setup"]},{"metadata":{"colab_type":"code","id":"QUdMK44zivu_","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","tf.enable_eager_execution()\n","\n","import tensorflow_hub as hub\n","import cv2\n","import numpy as np\n","import time\n","import glob"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"ax4iG21_lMfJ","colab":{}},"cell_type":"code","source":["from tensorflow.data import Dataset\n","import time\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"CyOJQ9ttywQg","colab":{}},"cell_type":"code","source":["from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.initializers import glorot_uniform\n","import tensorflow.keras.backend as K"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"9nvdi5EMk8FV","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"2rd05JvethaC"},"cell_type":"markdown","source":["# 2 Building the dataset (with tf.data.dataset API)"]},{"metadata":{"colab_type":"code","id":"JVx-ECCKk8kB","colab":{}},"cell_type":"code","source":["image_files= []\n","image_labels = []\n","\n","# maybe make more flexible by reading categories automatically from folder.\n","for index, category in enumerate(['car','plane','train']): \n","  \n","  category_image_files = (glob.glob(\"drive/My Drive/xylosai/tensorflow/images/{0}/*.jpg\".format(category)) \n","    +glob.glob(\"drive/My Drive/xylosai/tensorflow/images/{0}/*.jpeg\".format(category))\n","    +glob.glob(\"drive/My Drive/xylosai/tensorflow/images/{0}/*.png\".format(category)))\n","  image_files += category_image_files\n","  image_labels += ([index] * len(category_image_files))\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"uR2yOsoan_8n","colab":{}},"cell_type":"code","source":["# sanity check\n","print(len(image_files))\n","print(len(image_labels))\n","print(np.unique(image_labels))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"yfEvCVcKlaLD","colab":{}},"cell_type":"code","source":["# global variables\n","HEIGHT = 256\n","WIDTH = 256\n","CHANNELS = 3\n","NUM_CLASSES = 3\n","BATCH_SIZE = 20\n","EPOCHS = 5"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"HlqgEVojlIvQ","colab":{}},"cell_type":"code","source":["def _parse_function(filename,label):\n","  image_string = tf.read_file(filename)\n","  image = tf.image.decode_jpeg(image_string,channels=CHANNELS)\n","  return image, label\n","\n","def _resize(image, label):\n","  image_resized = tf.image.resize_images(image, [HEIGHT, WIDTH])\n","  return image_resized, label\n","\n","def _normalize(image, label):\n","  return tf.div(image,255), label\n","\n","def _expand(image, label):\n","  return tf.expand_dims(image, axis=0), label\n","\n","def _onehot(image, label):\n","  onehot = tf.one_hot(label,NUM_CLASSES)\n","#   onehot = tf.expand_dims(onehot, axis = 0)\n","  return image, onehot"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"a4LDEy6HKcjT","colab":{}},"cell_type":"code","source":["train_image_files, test_image_files, train_image_labels, test_image_labels = train_test_split(image_files, image_labels, test_size = 0.25, random_state = 0)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"csjgvA1HlSJK","colab":{}},"cell_type":"code","source":["# Dataset.from_tensor_slices requires data that fits entirely in memory. For larget datasets, for example use a generator with Dataset.from_generator\n","\n","train_dataset = Dataset.from_tensor_slices((train_image_files,train_image_labels)).map(_parse_function).map(_resize).map(_normalize).map(_onehot)#.map(_expand)\n","test_dataset = Dataset.from_tensor_slices((test_image_files,test_image_labels)).map(_parse_function).map(_resize).map(_normalize).map(_onehot)#.map(_expand)\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"22eFrdYKlS2Q","colab":{}},"cell_type":"code","source":["train_dataset.batch(20).output_shapes"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"N7zRyDyvxO3X"},"cell_type":"markdown","source":["# 3 Building and training the model (with Eager execution)"]},{"metadata":{"colab_type":"code","id":"lcp50ApCxOVj","colab":{}},"cell_type":"code","source":["def ModelCarPlaneTrain(input_shape = (256,256,3), classes = 3):\n","    \n","    X_input = Input(input_shape)\n","    \n","    X = Conv2D(160,(4,4),strides = (1,1),name='conv1',kernel_initializer = glorot_uniform(seed=0))(X_input)\n","    X = Activation('relu')(X)\n","    X = MaxPooling2D((8, 8))(X)\n","    \n","    X = Conv2D(160,(2,2),strides = (1,1),name='conv2',kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = Activation('relu')(X)\n","    X = MaxPooling2D((4, 4))(X)\n","    \n","    X = Conv2D(80,(2,2),strides = (1,1),name='conv3',kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = Activation('relu')(X)\n","    X = MaxPooling2D((4, 4))(X)\n","    \n","    X = Flatten()(X)\n","    X = Dense(classes, activation='softmax', name='fc', kernel_initializer = glorot_uniform(seed=0))(X)\n","    \n","    model = Model(inputs = X_input, outputs = X, name = 'ModelCarPlaneTrain')\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"G--6QVBjyONa","colab":{}},"cell_type":"code","source":["model = ModelCarPlaneTrain()\n","\n","optimizer = tf.train.AdamOptimizer()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"JFRTlvSEt1Gp","colab":{}},"cell_type":"code","source":["start_time = time.time()\n","\n","loss_history = []\n","\n","for (batch, (images, labels)) in enumerate(train_dataset.batch(BATCH_SIZE).repeat(EPOCHS)):\n","  if (batch % 1 == 0):\n","    print(batch)\n","  with tf.GradientTape() as tape:\n","    logits = model(images)\n","    #default reduction of this loss function is Reduction.SUM_BY_NONZERO_WEIGHTS\n","    #see documentation https://www.tensorflow.org/api_docs/python/tf/losses/Reduction\n","    #this is relevant for evaluating the model in Eager with the dataset API. \n","    loss_value = tf.losses.softmax_cross_entropy(labels, logits)\n","  \n","  loss_history.append(loss_value.numpy())\n","  grads = tape.gradient(loss_value, model.variables)\n","  optimizer.apply_gradients(zip(grads, model.variables), global_step = tf.train.get_or_create_global_step())\n","  \n","print(\"fitting took {0} seconds\".format(time.time()-start_time))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"U5fdxt9qzbpH","colab":{}},"cell_type":"code","source":["plt.plot(loss_history)\n","plt.xlabel('Batch #')\n","plt.ylabel('Loss [entropy]')"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"mGFipK-CNQEN","colab":{}},"cell_type":"code","source":["loss_value"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"GrPNfVPsd59m"},"cell_type":"markdown","source":["# 4 Evaluating the model"]},{"metadata":{"colab_type":"code","id":"10zCG-yHd8Yp","colab":{}},"cell_type":"code","source":["start_time = time.time()\n","\n","loss_history = []\n","\n","for (batch, (images, labels)) in enumerate(test_dataset.batch(BATCH_SIZE)):\n","  if (batch % 1 == 0):\n","    print(batch)\n","    logits = model(images)\n","    #default reduction of this loss function is Reduction.SUM_BY_NONZERO_WEIGHTS\n","    #see documentation https://www.tensorflow.org/api_docs/python/tf/losses/Reduction\n","    #this is relevant for evaluating the model in Eager with the dataset API. \n","    loss_value = tf.losses.softmax_cross_entropy(labels, logits)\n","  \n","  loss_history.append(loss_value.numpy())\n","\n","#This is mathematically correct thanks to Reduction.SUM_BY_NONZERO_WEIGHTS\n","mean_loss = sum(loss_history)/float(len(loss_history)) \n","  \n","print(\"evaluating took {0} seconds\".format(time.time()-start_time))\n","print(\"mean loss is {0}\".format(mean_loss))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"t8qBiSPegtgT","colab":{}},"cell_type":"code","source":["# not a very useful plot. We do not expect the loss to decrease wile evaluating\n","plt.plot(loss_history)\n","plt.xlabel('Batch #')\n","plt.ylabel('Loss [entropy]')"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"dVZ-4pYxhat2","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"classifier-convNet.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"python352","language":"python","name":"python352"}},"cells":[{"metadata":{"id":"f36YIVpsqC3U","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.python.framework import ops\n","import matplotlib.pyplot as plt\n","import cv2\n","import glob\n","import os\n","import numpy as np\n","import math\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","\n","from matplotlib.pyplot import imshow\n","%matplotlib inline\n","\n","from tensorflow.contrib.tensorboard.plugins import projector\n","import math\n","\n","import myImageLibrary\n","\n","from tensorflow.saved_model import simple_save"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TtSjj6jdqC3b","colab_type":"code","colab":{}},"cell_type":"code","source":["def convert_to_one_hot(Y, C):\n","    Y = np.eye(C)[Y.reshape(-1)].T\n","    return Y"],"execution_count":0,"outputs":[]},{"metadata":{"collapsed":true,"id":"eZwpkMivqC3f","colab_type":"raw"},"cell_type":"markdown","source":["def showimage(title,img):\n","    cv2.imshow(title,img)\n","    cv2.waitKey(0)\n","    cv2.destroyAllWindows()\n","    \n","def crop_center(image,cropx,cropy):\n","    w = image.shape[1]\n","    h = image.shape[0]\n","    startx = w//2-(cropx//2)\n","    starty = h//2-(cropy//2)\n","    return image[starty:starty+cropy, startx:startx+cropx]\n","\n","def resize_crop(image,square_size):\n","    w = image.shape[1]\n","    h = image.shape[0]\n","    min_dim = min(w,h)  \n","    max_square_image = crop_center(image, min_dim, min_dim)\n","    result = cv2.resize(max_square_image,(square_size,square_size),0,0)\n","    return result\n","\n","\n","\n"]},{"metadata":{"id":"QDzED77MqC3g","colab_type":"code","colab":{}},"cell_type":"code","source":["def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n","    \"\"\"\n","    Creates a list of random minibatches from (X, Y)\n","    \n","    Arguments:\n","    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n","    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n","    mini_batch_size - size of the mini-batches, integer\n","    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n","    \n","    Returns:\n","    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n","    \"\"\"\n","    \n","    m = X.shape[0]                  # number of training examples\n","    mini_batches = []\n","    np.random.seed(seed)\n","    \n","    # Step 1: Shuffle (X, Y)\n","    permutation = list(np.random.permutation(m))\n","    shuffled_X = X[permutation,:,:,:]\n","    shuffled_Y = Y[permutation,:]\n","\n","    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n","    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n","    for k in range(0, num_complete_minibatches):\n","        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n","        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","    \n","    # Handling the end case (last mini-batch < mini_batch_size)\n","    if m % mini_batch_size != 0:\n","        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n","        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","    \n","    return mini_batches\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jicbrdGRqC3j","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_classifier_data(image_folder, size = 256, verbose = True, mode = \"channels_first\",one_hot = False):\n","    \n","    folders = os.listdir(\"images\")\n","    if verbose:\n","        print(\"classes: {0}\".format(folders))\n","    input_labels = []\n","    input_data = []\n","    label_dict = {}\n","\n","    for i, folder in enumerate(folders):\n","        if verbose:\n","            print(\"{0}..\".format(folder))   \n","        image_list = myImageLibrary.get_images(os.path.join(\"images\",folder))\n","        if mode == \"channels_first\":\n","            processed_image_list =  [np.around(myImageLibrary.resize_crop(image,size).transpose(2,0,1)/255.0,decimals=12) for image in image_list]\n","        elif mode == \"channels_last\":\n","            processed_image_list =  [np.around(myImageLibrary.resize_crop(image,size)/255.0,decimals=12) for image in image_list]\n","        else:\n","            print(\"invalid mode. pick channels_first or channels_last\")\n","        # processed = normalized, resized and cropped, transposed to \"channels first\"\n","        input_labels = input_labels+([i]*len(processed_image_list))\n","        input_data = input_data+processed_image_list\n","        label_dict[str(i)] = folder\n","        \n","    shape = list(input_data[0].shape)\n","    shape[:0] = [len(input_data)]\n","    input_array = np.concatenate(input_data).reshape(shape)\n","    \n","    input_labels = np.array(input_labels)\n","    \n","    if one_hot:\n","        input_labels = convert_to_one_hot(np.array(input_labels),len(folders)).T\n","            \n","    return input_array, input_labels, label_dict\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"ojJLUsmzqC3m","colab_type":"code","colab":{}},"cell_type":"code","source":["image_folder = \"images\"\n","input_images, labels, label_dict = get_classifier_data(image_folder,mode=\"channels_last\",one_hot = True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RxqftsovqC3p","colab_type":"code","colab":{}},"cell_type":"code","source":["print(input_images.shape)\n","print(labels.shape)\n","print(label_dict)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"23Ec3OEsqC3s","colab_type":"code","colab":{}},"cell_type":"code","source":["X_train, X_test, Y_train, Y_test = train_test_split(input_images,labels)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tUgj9UHGqC3v","colab_type":"raw"},"cell_type":"markdown","source":["result_car = glob.glob('fotos_auto/*.jpg')+glob.glob('fotos_auto/*.jpeg')+glob.glob('fotos_auto/*.png')\n","images_car = [cv2.imread(item) for item in result_car]\n","print(len(images_car))\n","images_car = images_car[1:] #because first item appears to be None\n","\n","result_plane = glob.glob('fotos_vliegtuig/*.jpg')+glob.glob('fotos_vliegtuig/*.jpeg')+glob.glob('fotos_vliegtuig/*.png')\n","images_plane = [cv2.imread(item) for item in result_plane]\n","print(len(images_plane))\n","\n","result_train = glob.glob('fotos_trein/*.jpg')+glob.glob('fotos_trein/*.jpeg')+glob.glob('fotos_trein/*.png')\n","images_train = [cv2.imread(item) for item in result_train]\n","print(len(images_train))"]},{"metadata":{"collapsed":true,"id":"gOqwxapMqC3x","colab_type":"raw"},"cell_type":"markdown","source":["average_min_dim = 256\n","\n","# rescaling and cropping images\n","\n","resized_images_car = np.array([resize_crop(item,average_min_dim) for item in images_car])\n","resized_images_plane = np.array([resize_crop(item,average_min_dim) for item in images_plane])\n","resized_images_train = np.array([resize_crop(item,average_min_dim) for item in images_train])"]},{"metadata":{"collapsed":true,"id":"008BZydBqC3x","colab_type":"raw"},"cell_type":"markdown","source":["# combine all data, labels, normalize and make a train-test split\n","\n","input_images = np.append(np.append(resized_images_car,resized_images_plane,axis=0),resized_images_train,axis=0)/255\n","labels = np.array([0] * resized_images_car.shape[0] + [1] * resized_images_plane.shape[0] + [2] * resized_images_train.shape[0])\n","labels = np.reshape(labels,(-1,1))\n","outputy = convert_to_one_hot(labels,3).T\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(input_images,outputy)\n"]},{"metadata":{"id":"1yQtI6NpqC3z","colab_type":"code","colab":{}},"cell_type":"code","source":["X_train.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-4gxGRHBqC31","colab_type":"code","colab":{}},"cell_type":"code","source":["outputy.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Zbgl_UZHqC34","colab_type":"code","colab":{}},"cell_type":"code","source":["X_train"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tPaKzbz_qC37","colab_type":"code","colab":{}},"cell_type":"code","source":["def create_sprite(X_train, sprite_path='./', p = 80):\n","    m = X_train.shape[0]\n","    #sprite will consist of n rows, n columns of mini-pictures, ordered row-first. Last row can be incomplete\n","    n = int(math.sqrt(m))+1\n","    assert n*80 <= 8192\n","    sprite = np.zeros((n*80,n*80,3))\n","    i = 0\n","    j = 0\n","\n","    for r in range(m):\n","        \n","        sprite[i:i+p,j:j+p,:] = cv2.resize(X_train[r,:,:,:],(p,p))\n","        #cv2.imwrite('debug.jpg',cv2.resize(X_train[r,:,:,:],(p,p)))\n","        if (r+1)%n != 0:\n","            j += p\n","        else:\n","            j = 0\n","            i += p\n","    sprite = (sprite*255).astype(int)\n","    #weird: showimage requires floats from 0 to 1 (otherwise output is black)\n","    #but for imwrite, values must be from 0 to 255 (otherwise, outputfile is entirely black)\n","    #showimage(\"sprite\",sprite)\n","    cv2.imwrite(os.path.join(sprite_path,'sprite.jpg'),sprite)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8m_qIocaqC3_","colab_type":"code","colab":{}},"cell_type":"code","source":["# GRADED FUNCTION: create_placeholders\n","\n","def create_placeholders(n_H0, n_W0, n_C0, n_y):\n","    \"\"\"\n","    Creates the placeholders for the tensorflow session.\n","    \n","    Arguments:\n","    n_H0 -- scalar, height of an input image\n","    n_W0 -- scalar, width of an input image\n","    n_C0 -- scalar, number of channels of the input\n","    n_y -- scalar, number of classes\n","        \n","    Returns:\n","    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n","    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n","    \"\"\"\n","\n","    ### START CODE HERE ### (≈2 lines)\n","    X = tf.placeholder(tf.float32, shape=(None,n_H0,n_W0,n_C0))\n","    Y = tf.placeholder(tf.float32, shape=(None,n_y))\n","    ### END CODE HERE ###\n","    \n","    return X, Y\n","\n","# GRADED FUNCTION: initialize_parameters\n","\n","def initialize_parameters():\n","    \"\"\"\n","    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n","                        W1 : [4, 4, 3, 8]\n","                        W2 : [2, 2, 8, 16]\n","    Returns:\n","    parameters -- a dictionary of tensors containing W1, W2\n","    \"\"\"\n","    \n","    tf.set_random_seed(1)                   \n","        \n","\n","    with tf.name_scope(\"conv1\"):\n","        W1 = tf.get_variable(\"W1\",[4,4,3,8],initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n","    with tf.name_scope(\"conv2\"):\n","        W2 = tf.get_variable(\"W2\",[2, 2, 8, 16],initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n","\n","\n","    parameters = {\"W1\": W1,\n","                  \"W2\": W2}\n","    \n","    return parameters"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GIZ9OmTpqC4D","colab_type":"code","colab":{}},"cell_type":"code","source":["# GRADED FUNCTION: forward_propagation\n","\n","def forward_propagation(X, parameters):\n","    \"\"\"\n","    Implements the forward propagation for the model:\n","    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n","    \n","    Arguments:\n","    X -- input dataset placeholder, of shape (input size, number of examples)\n","    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n","                  the shapes are given in initialize_parameters\n","\n","    Returns:\n","    Z3 -- the output of the last LINEAR unit\n","    \"\"\"\n","    \n","    # Retrieve the parameters from the dictionary \"parameters\" \n","    W1 = parameters['W1']\n","    W2 = parameters['W2']\n","    with tf.name_scope(\"conv1\"):\n","    # CONV2D: stride of 1, padding 'SAME'\n","        Z1 = tf.nn.conv2d(X, W1, strides = [1,1,1,1], padding = 'SAME')\n","        # RELU\n","        A1 = tf.nn.relu(Z1)\n","    with tf.name_scope(\"maxpool1\"):\n","        # MAXPOOL: window 8x8, sride 8, padding 'SAME'\n","        P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')\n","    with tf.name_scope(\"conv2\"):    \n","        # CONV2D: filters W2, stride 1, padding 'SAME'\n","        Z2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = 'SAME')\n","        # RELU\n","        A2 = tf.nn.relu(Z2)\n","    with tf.name_scope(\"maxpool2\"):\n","        # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n","        P2 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')\n","    with tf.name_scope(\"dense\"):\n","        # FLATTEN\n","        P3 = tf.contrib.layers.flatten(P2)\n","        # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n","        # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n","        Z3 = tf.contrib.layers.fully_connected(P3, 3, activation_fn = None)\n","    \n","\n","    return Z3, P2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C_ws7iIRqC4G","colab_type":"code","colab":{}},"cell_type":"code","source":["def compute_cost(Z3, Y):\n","    \"\"\"\n","    Computes the cost\n","    \n","    Arguments:\n","    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n","    Y -- \"true\" labels vector placeholder, same shape as Z3\n","    \n","    Returns:\n","    cost - Tensor of the cost function\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (1 line of code)\n","    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\n","    ### END CODE HERE ###\n","    \n","    return cost"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RS_dIyaeqC4I","colab_type":"code","colab":{}},"cell_type":"code","source":["def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\n","          num_epochs = 100, minibatch_size = 64, print_cost = True,tensorboardpath=None,saved_model_path = None):\n","    \"\"\"\n","    Implements a three-layer ConvNet in Tensorflow:\n","    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n","    \n","    Arguments:\n","    X_train -- training set, of shape (None, 64, 64, 3)\n","    Y_train -- test set, of shape (None, n_y = 6)\n","    X_test -- training set, of shape (None, 64, 64, 3)\n","    Y_test -- test set, of shape (None, n_y = 6)\n","    learning_rate -- learning rate of the optimization\n","    num_epochs -- number of epochs of the optimization loop\n","    minibatch_size -- size of a minibatch\n","    print_cost -- True to print the cost every 100 epochs\n","    \n","    Returns:\n","    train_accuracy -- real number, accuracy on the train set (X_train)\n","    test_accuracy -- real number, testing accuracy on the test set (X_test)\n","    parameters -- parameters learnt by the model. They can then be used to predict.\n","    \"\"\"\n","    \n","    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n","    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n","    seed = 3                                          # to keep results consistent (numpy seed)\n","    (m, n_H0, n_W0, n_C0) = X_train.shape             \n","    n_y = Y_train.shape[1]                            \n","    costs = []                                        # To keep track of the cost\n","    \n","    \n","    # Create Placeholders of the correct shape\n","    ### START CODE HERE ### (1 line)\n","    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n","    ### END CODE HERE ###\n","\n","    # Initialize parameters\n","    ### START CODE HERE ### (1 line)\n","    parameters = initialize_parameters()\n","    ### END CODE HERE ###\n","    \n","    # Forward propagation: Build the forward propagation in the tensorflow graph\n","    ### START CODE HERE ### (1 line)\n","    Z3, P2 = forward_propagation(X, parameters)\n","    ### END CODE HERE ###\n","    \n","    # Cost function: Add cost function to tensorflow graph\n","    cost = compute_cost(Z3, Y)\n","\n","    if tensorboardpath is not None:\n","        summary_cost = tf.summary.scalar('cost',cost)\n","        summary_output = tf.summary.histogram('output',Z3)\n","        merged_summary = tf.summary.merge_all()\n","        \n","        summary_images = tf.summary.image('training_images',X)\n","    \n","    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n","    ### START CODE HERE ### (1 line)\n","    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n","    #optimizer is an 'Operation' that updates the variables\n","    ### END CODE HERE ###\n","    \n","    # Initialize all the variables globally\n","    init = tf.global_variables_initializer()\n","     \n","    # Start the session to compute the tensorflow graph    \n","    with tf.Session() as sess:\n","        \n","        \n","\n","        # Run the initialization\n","        sess.run(init)\n","        \n","        \n","        if tensorboardpath is not None:\n","            filewriter_train = tf.summary.FileWriter(os.path.join(tensorboardpath,'train'),sess.graph)\n","            filewriter_test = tf.summary.FileWriter(os.path.join(tensorboardpath,'test'),sess.graph)\n","            #filewriter_embedding = tf.summary.FileWriter('./tensorboard_embedding/',sess.graph)\n","            \n","            image_summary = sess.run(summary_images,feed_dict={X: X_train, Y: Y_train})\n","            filewriter_train.add_summary(image_summary)\n","            saver = tf.train.Saver()\n","\n","        \n","        # Do the training loop\n","        for epoch in range(num_epochs):\n","\n","            minibatch_cost = 0.\n","            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n","            seed = seed + 1\n","            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n","\n","            for i,minibatch in enumerate(minibatches):\n","\n","                # Select a minibatch\n","                (minibatch_X, minibatch_Y) = minibatch\n","                # IMPORTANT: The line that runs the graph on a minibatch.\n","                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n","                ### START CODE HERE ### (1 line)\n","                \n","                if tensorboardpath is not None:\n","                    summary,_ , temp_cost = sess.run([summary_cost,optimizer,cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n","                    filewriter_train.add_summary(summary,epoch*num_minibatches+i)\n","                else:\n","                    _ , temp_cost = sess.run([optimizer,cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n","                \n","                ### END CODE HERE ###\n","                \n","                minibatch_cost += temp_cost / num_minibatches\n","                \n","                #evaluate train cost every MINIBATCH and display in tensorboard\n","                #if tensorboardpath is not None:\n","                #    filewriter_train.add_summary(summary,epoch*num_minibatches+i)\n","                \n","                \n","            # Print the cost every epoch\n","            if print_cost == True and epoch % 5 == 0:\n","                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n","            if print_cost == True and epoch % 1 == 0:\n","                costs.append(minibatch_cost)\n","                \n","            #evaluate test cost and test output histogram every EPOCH and display in tensorboard\n","            if tensorboardpath is not None:           \n","                summary = sess.run(merged_summary, feed_dict={X: X_test, Y: Y_test})\n","                filewriter_test.add_summary(summary,(epoch+1)*num_minibatches)\n","        \n","                print(\"setting up tensorboard...\")\n","                LOG_DIR = os.path.join(tensorboardpath,\"test\")\n","        \n","                z3 = sess.run(Z3,feed_dict={X: X_test, Y: Y_test})\n","                embedding_var = tf.Variable(z3,  name='embedding')\n","                #embedding_var is initialized with z3 values. Running this initialization in line below.\n","                sess.run(embedding_var.initializer)\n","                summary_writer = tf.summary.FileWriter(LOG_DIR)\n","        \n","                config = projector.ProjectorConfig()\n","                embedding = config.embeddings.add()     \n","                embedding.tensor_name = embedding_var.name\n","        \n","                print(\"creating sprite...\")\n","                p=80 #thumbnail pixel size in sprite image.\n","                create_sprite(X_test,sprite_path=LOG_DIR,p=p)        \n","                #the path below must be relative to the directory of the filewriter. It is for example correct, if \n","                #the path is sprite.jpg in the configfile, and configfile and sprite.jpg are in same folder\n","                embedding.sprite.image_path = \"sprite.jpg\"\n","                print(embedding.sprite.image_path)       \n","                #Specify the width and height of a single thumbnail.\n","                embedding.sprite.single_image_dim.extend([p, p])\n","\n","        # Calculate the correct predictions\n","        predict_op = tf.argmax(Z3, 1)\n","        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n","        \n","        # Calculate accuracy on the test set\n","        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n","        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n","        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n","        #t.eval(feed_dict=None, session=None) is a shortcut for tf.get_default_session().run(t)\n","        print(\"Train Accuracy:\", train_accuracy)\n","        print(\"Test Accuracy:\", test_accuracy)\n","        \n","        if tensorboardpath is not None:\n","            print(\"creating metadata for embedding visualisation...\")\n","            digit_to_label = {\"0\":\"car\",\"1\":\"plane\",\"2\":\"train\"}\n","            with open(os.path.join(LOG_DIR,\"metadata.tsv\"), 'w') as f:\n","                f.write(\"Index\\tLabel\\n\")\n","                print(z3.shape)\n","                print(np.argmax(z3,axis=1).astype(int).shape)\n","                for index, digit in enumerate(list(np.argmax(z3,axis=1).astype(int))):\n","                    f.write(\"{0}\\t{1}\\n\".format(index,digit_to_label[str(digit)])) \n","            embedding.metadata_path = \"metadata.tsv\"\n","\n","            projector.visualize_embeddings(filewriter_test,config)\n","            #projector.visualize_embeddings(summary_writer,config)\n","            saver = tf.train.Saver([embedding_var])\n","            saver.save(sess,os.path.join(LOG_DIR,\"model.ckpt\"))\n","\n","        \n","        # plot the cost\n","        plt.plot(np.squeeze(costs))\n","        plt.ylabel('cost')\n","        plt.xlabel('iterations (per tens)')\n","        plt.title(\"Learning rate =\" + str(learning_rate))\n","        plt.show()\n","        \n","        if saved_model_path is not None:\n","            simple_save(sess,saved_model_path,inputs={\"X\":X},outputs={\"Z3\":Z3})\n","                \n","        return train_accuracy, test_accuracy, parameters"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TASnGUOWqC4O","colab_type":"text"},"cell_type":"markdown","source":["Documentation for tensorboard add_summary: https://www.tensorflow.org/versions/r1.5/api_docs/python/tf/summary/FileWriter#add_summary\n","\n","**You can pass the result of evaluating any SUMMARY op, using tf.Session.run or tf.Tensor.eval, to this function**\n","\n","Thus, you pass the result of evaluating a SUMMARY OPERATION, not just the result of evaluating an ordinary tensor. Type of summary variable is *bytes*, which can be passed to filewriter. Merging all can be handy, tf.summary.merge_all() is also a summary op."]},{"metadata":{"id":"Gnl1fPCQqC4Q","colab_type":"code","colab":{}},"cell_type":"code","source":["run_id"],"execution_count":0,"outputs":[]},{"metadata":{"collapsed":true,"id":"FqGmP4KQqC4V","colab_type":"text"},"cell_type":"markdown","source":["# conclusions\n","\n","High variance: train accuracy 100%, test accuracy approx. 80%\n","\n","Randomness in results of test accuracy: multiple percentage points! How is this possible??\n"]},{"metadata":{"scrolled":true,"id":"QKz63a70qC4X","colab_type":"code","colab":{}},"cell_type":"code","source":["#with tensorboard:\n","#run_id += 1\n","#_, _, parameters, sess = model(X_train, Y_train, X_test, Y_test,num_epochs=10,tensorboardpath=\"./tensorboard/{0}/\".format(run_id))\n","\n","EXPORT_DIR = \"savedModel\"\n","\n","#without tensorboard, but with saving to SavedModel for serving\n","_, _, parameters, sess = model(X_train, Y_train, X_test, Y_test,num_epochs=10,saved_model_path = EXPORT_DIR)"],"execution_count":0,"outputs":[]}]}